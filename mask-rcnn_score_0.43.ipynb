{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil \n",
    "from zipfile import ZipFile\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import skimage\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "import config \n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "#import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"checkpoints.h5\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "#if not os.path.exists(COCO_MODEL_PATH):\n",
    "    #utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "AUGMENTATION_FLIP_LR           0.5\n",
      "AUGMENTATION_FLIP_UD           0.5\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_SHAPES                [[144 144]\n",
      " [ 72  72]\n",
      " [ 36  36]\n",
      " [ 18  18]\n",
      " [  9   9]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COCO_MODEL_PATH                mask_rcnn_coco.h5\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  576\n",
      "IMAGE_MIN_DIM                  576\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [576 576   3]\n",
      "LEARNING_RATE                  0.0001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               256\n",
      "MEAN_PIXEL                     [42.17746161 38.21568456 46.82167803]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MODEL_DIR                      checkpoints\n",
      "NAME                           nuclei\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.6\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    320\n",
      "STEPS_PER_EPOCH                300\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               35\n",
      "WEIGHT_DECAY                   0.0001\n",
      "bs                             2\n",
      "init_with                      last\n",
      "test_data_root                 stage1_test/\n",
      "train_data_root                train/\n",
      "val_data_root                  train/\n",
      "\n",
      "\n",
      "\n",
      "Configurations:\n",
      "AUGMENTATION_FLIP_LR           0.5\n",
      "AUGMENTATION_FLIP_UD           0.5\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_SHAPES                [[144 144]\n",
      " [ 72  72]\n",
      " [ 36  36]\n",
      " [ 18  18]\n",
      " [  9   9]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COCO_MODEL_PATH                mask_rcnn_coco.h5\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  576\n",
      "IMAGE_MIN_DIM                  576\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [576 576   3]\n",
      "LEARNING_RATE                  0.0001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               256\n",
      "MEAN_PIXEL                     [42.17746161 38.21568456 46.82167803]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MODEL_DIR                      checkpoints\n",
      "NAME                           nuclei\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.6\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    320\n",
      "STEPS_PER_EPOCH                300\n",
      "TRAIN_ROIS_PER_IMAGE           512\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               35\n",
      "WEIGHT_DECAY                   0.0001\n",
      "bs                             2\n",
      "init_with                      last\n",
      "test_data_root                 stage1_test/\n",
      "train_data_root                train/\n",
      "val_data_root                  train/\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Config2(config.Config):\n",
    "\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"nuclei\"\n",
    "    train_data_root = 'train/' \n",
    "    val_data_root = 'train/'\n",
    "    test_data_root = 'stage1_test/'\n",
    "    \n",
    "    MODEL_DIR = 'checkpoints'\n",
    "    COCO_MODEL_PATH = 'mask_rcnn_coco.h5'\n",
    "    # imagenet, coco, or last\n",
    "    init_with = \"last\"  \n",
    "    \n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. Batch size is GPUs * images/GPU.\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 2\n",
    "    bs = GPU_COUNT * IMAGES_PER_GPU\n",
    "    # Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch.\n",
    "    # typically be equal to the number of samples of your dataset divided by the batch size\n",
    "    STEPS_PER_EPOCH = 300\n",
    "    VALIDATION_STEPS = 70//bs\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + nucleis\n",
    "\n",
    "    # Input image resing\n",
    "    # Images are resized such that the smallest side is >= IMAGE_MIN_DIM and\n",
    "    # the longest side is <= IMAGE_MAX_DIM. In case both conditions can't\n",
    "    # be satisfied together the IMAGE_MAX_DIM is enforced.\n",
    "    IMAGE_MIN_DIM = 576\n",
    "    IMAGE_MAX_DIM = 576\n",
    "   # If True, pad images with zeros such that they're (max_dim by max_dim)\n",
    "    IMAGE_PADDING = True  # currently, the False option is not supported\n",
    "\n",
    "\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels, maybe add a 256?\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE = 'resnet101'\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 320 #300\n",
    "    \n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "    MASK_SHAPE = [28, 28]\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # (increasing) the RPN NMS threshold.\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 512\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can reduce(increase?) this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.6\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 256\n",
    "    \n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 400 \n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7 # may be smaller?\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3 # 0.3\n",
    "    \n",
    "    \n",
    "    MEAN_PIXEL = np.array([42.17746161,38.21568456,46.82167803])\n",
    "    \n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    \n",
    "    \n",
    "opt = Config2()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt.display()\n",
    "    STEPS_PER_EPOCH = 300\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 40\n",
    "    \n",
    "    LEARNING_RATE = 0.0001\n",
    "    \n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)\n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 400 \n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7 # may be smaller?\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3 # 0.3\n",
    "    \n",
    "    MAX_GT_INSTANCES = 550\n",
    "    \n",
    "    \n",
    "config = opt\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Add classes\n",
    "class NucleiDataset(utils.Dataset):\n",
    "    \n",
    "    def load_nuclei(self, root_path, mode='train', filter_ids=None):\n",
    "        \n",
    "        self.add_class(\"nuclei\", 1, \"nucleus\")\n",
    "        \n",
    "        if mode == 'inference':\n",
    "            files = os.listdir(os.path.join(root_path, 'test_imgs'))\n",
    "        else:\n",
    "            files = os.listdir(os.path.join(root_path, 'train_imgs'))\n",
    "\n",
    "        if filter_ids is not None:\n",
    "            files = [item for item in files if item.split('.')[0] in filter_ids]\n",
    "            \n",
    "        for i, ffile in enumerate(tqdm(files)):\n",
    "            if mode == 'inference':\n",
    "                data_path = os.path.join(root_path, 'test_imgs', ffile)\n",
    "                mask_path = 'No masks'\n",
    "            else:\n",
    "                data_path = os.path.join(root_path, 'train_imgs', ffile)\n",
    "                \n",
    "            bg_color = random.randint(0, 255)\n",
    "            original_id = ffile.split('.')[0]\n",
    "            # TODO change later maybe\n",
    "\n",
    "            data = cv2.imread(data_path)[:, :, 0]\n",
    "            height, width = data.shape\n",
    "\n",
    "            self.add_image(\"nuclei\", image_id=i, path=data_path,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, original_id=original_id,\n",
    "                           data_path=data_path,\n",
    "                           root_path=root_path)\n",
    "\n",
    "            image = cv2.imread(data_path)\n",
    "            # If grayscale. Convert to RGB for consistency.\n",
    "            if image.ndim != 3:\n",
    "                image = skimage.color.gray2rgb(image)\n",
    "        \n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        info = self.get_info(image_id)\n",
    "        path = info['data_path']\n",
    "        image = cv2.imread(path)\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if image.ndim != 3:\n",
    "            image = skimage.color.gray2rgb(image)\n",
    "        return image\n",
    "\n",
    "    def get_info(self, image_id):\n",
    "        return self.image_info[image_id]\n",
    "        \n",
    "    def load_mask(self, image_id):\n",
    "        info = self.get_info(image_id)\n",
    "        original_id = info['original_id']\n",
    "        root_path = info['root_path']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        return self.add_mask_data(original_id, root_path, width=width, height=height)[:]\n",
    "\n",
    "    def add_mask_data(self, original_id, root_path, width=256, height=256):\n",
    "        # info = self.image_info[image_id]\n",
    "        orifinal_id = original_id\n",
    "        root_path = root_path\n",
    "        \n",
    "        all_masks = os.listdir(os.path.join(root_path, 'masks'))\n",
    "        all_masks = [element for element in all_masks if element.split('_')[0] == orifinal_id]\n",
    "\n",
    "        num_labels = 0\n",
    "        #mask_ids = next(os.walk(all_masks))[1]\n",
    "        for i, ffile in enumerate(all_masks): \n",
    "            subpath = os.path.join(root_path, 'masks', ffile)\n",
    "            data = imread(subpath)[:, :]\n",
    "            if np.sum(data) != 0:\n",
    "                num_labels += 1\n",
    "\n",
    "        mask = np.zeros([height,width,num_labels], dtype=np.bool)\n",
    "        num_labels = 0\n",
    "        for i, ffile in enumerate(all_masks): \n",
    "            subpath = os.path.join(root_path, 'masks', ffile)\n",
    "            data = imread(subpath)[:, :]\n",
    "\n",
    "            if np.sum(data) != 0:\n",
    "                data = (data != 0)\n",
    "                mask[:, :, num_labels] = data\n",
    "                num_labels += 1\n",
    "        \n",
    "        class_ids = np.array([1] * num_labels)\n",
    "        class_ids = class_ids.astype(np.int32)\n",
    "        return mask, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 598/598 [00:04<00:00, 120.16it/s]\n",
      "100%|██████████| 598/598 [00:04<00:00, 134.01it/s]\n"
     ]
    }
   ],
   "source": [
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                            \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "filter_ids = os.listdir(os.path.join(ROOT_DIR, 'train_imgs'))\n",
    "files = [item.split('.')[0] for item in filter_ids]\n",
    "original_id = os.listdir(os.path.join(ROOT_DIR, 'masks'))\n",
    "all_masks = [element.split('_')[0] for element in original_id]\n",
    "        \n",
    "\n",
    "\n",
    "# Training dataset\n",
    "dataset_train = NucleiDataset()\n",
    "dataset_train.load_nuclei(ROOT_DIR, mode='training',filter_ids = files[:598] )\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = NucleiDataset()\n",
    "dataset_val.load_nuclei(ROOT_DIR, mode = 'training', filter_ids = files[66:] )\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 598\n",
      "Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. nucleus                                           \n"
     ]
    }
   ],
   "source": [
    "print(\"Image Count: {}\".format(len(dataset_train.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_train.num_classes))\n",
    "for i, info in enumerate(dataset_train.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights('checkpoints.h5/nuclei20180327T2044/mask_rcnn_nuclei_0057.h5', by_name=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 58. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /home/ec2-user/checkpoints.h5/nuclei20180327T2044/mask_rcnn_nuclei_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/dist-packages/scipy/ndimage/interpolation.py:616: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 794s 3s/step - loss: 0.2689 - rpn_class_loss: 0.0023 - rpn_bbox_loss: 0.0452 - mrcnn_class_loss: 0.0212 - mrcnn_bbox_loss: 0.0402 - mrcnn_mask_loss: 0.1601 - val_loss: 0.2964 - val_rpn_class_loss: 9.3338e-05 - val_rpn_bbox_loss: 0.0493 - val_mrcnn_class_loss: 0.0021 - val_mrcnn_bbox_loss: 0.0506 - val_mrcnn_mask_loss: 0.1942\n",
      "Epoch 60/69\n",
      "300/300 [==============================] - 706s 2s/step - loss: 0.2769 - rpn_class_loss: 0.0022 - rpn_bbox_loss: 0.0467 - mrcnn_class_loss: 0.0220 - mrcnn_bbox_loss: 0.0429 - mrcnn_mask_loss: 0.1631 - val_loss: 0.3807 - val_rpn_class_loss: 8.1784e-04 - val_rpn_bbox_loss: 0.0996 - val_mrcnn_class_loss: 0.0011 - val_mrcnn_bbox_loss: 0.0658 - val_mrcnn_mask_loss: 0.2135\n",
      "Epoch 61/69\n",
      "300/300 [==============================] - 709s 2s/step - loss: 0.2677 - rpn_class_loss: 0.0023 - rpn_bbox_loss: 0.0447 - mrcnn_class_loss: 0.0211 - mrcnn_bbox_loss: 0.0397 - mrcnn_mask_loss: 0.1599 - val_loss: 0.3079 - val_rpn_class_loss: 3.9046e-05 - val_rpn_bbox_loss: 0.0621 - val_mrcnn_class_loss: 0.0057 - val_mrcnn_bbox_loss: 0.0460 - val_mrcnn_mask_loss: 0.1940\n",
      "Epoch 62/69\n",
      "300/300 [==============================] - 710s 2s/step - loss: 0.2632 - rpn_class_loss: 0.0023 - rpn_bbox_loss: 0.0440 - mrcnn_class_loss: 0.0193 - mrcnn_bbox_loss: 0.0391 - mrcnn_mask_loss: 0.1586 - val_loss: 0.3184 - val_rpn_class_loss: 5.6921e-04 - val_rpn_bbox_loss: 0.0749 - val_mrcnn_class_loss: 0.0035 - val_mrcnn_bbox_loss: 0.0519 - val_mrcnn_mask_loss: 0.1875\n",
      "Epoch 63/69\n",
      "300/300 [==============================] - 709s 2s/step - loss: 0.2697 - rpn_class_loss: 0.0028 - rpn_bbox_loss: 0.0465 - mrcnn_class_loss: 0.0199 - mrcnn_bbox_loss: 0.0405 - mrcnn_mask_loss: 0.1601 - val_loss: 0.3097 - val_rpn_class_loss: 6.3005e-05 - val_rpn_bbox_loss: 0.0672 - val_mrcnn_class_loss: 9.1555e-04 - val_mrcnn_bbox_loss: 0.0490 - val_mrcnn_mask_loss: 0.1925\n",
      "Epoch 64/69\n",
      "300/300 [==============================] - 708s 2s/step - loss: 0.2496 - rpn_class_loss: 0.0020 - rpn_bbox_loss: 0.0394 - mrcnn_class_loss: 0.0181 - mrcnn_bbox_loss: 0.0367 - mrcnn_mask_loss: 0.1535 - val_loss: 0.3270 - val_rpn_class_loss: 2.6520e-04 - val_rpn_bbox_loss: 0.0786 - val_mrcnn_class_loss: 0.0023 - val_mrcnn_bbox_loss: 0.0519 - val_mrcnn_mask_loss: 0.1939\n",
      "Epoch 65/69\n",
      "300/300 [==============================] - 713s 2s/step - loss: 0.2571 - rpn_class_loss: 0.0019 - rpn_bbox_loss: 0.0426 - mrcnn_class_loss: 0.0189 - mrcnn_bbox_loss: 0.0381 - mrcnn_mask_loss: 0.1556 - val_loss: 0.3236 - val_rpn_class_loss: 7.6912e-05 - val_rpn_bbox_loss: 0.0701 - val_mrcnn_class_loss: 0.0037 - val_mrcnn_bbox_loss: 0.0545 - val_mrcnn_mask_loss: 0.1953\n",
      "Epoch 66/69\n",
      "300/300 [==============================] - 708s 2s/step - loss: 0.2507 - rpn_class_loss: 0.0024 - rpn_bbox_loss: 0.0389 - mrcnn_class_loss: 0.0189 - mrcnn_bbox_loss: 0.0362 - mrcnn_mask_loss: 0.1542 - val_loss: 0.3293 - val_rpn_class_loss: 1.1186e-04 - val_rpn_bbox_loss: 0.0703 - val_mrcnn_class_loss: 6.4806e-04 - val_mrcnn_bbox_loss: 0.0537 - val_mrcnn_mask_loss: 0.2045\n",
      "Epoch 67/69\n",
      "300/300 [==============================] - 713s 2s/step - loss: 0.2585 - rpn_class_loss: 0.0024 - rpn_bbox_loss: 0.0422 - mrcnn_class_loss: 0.0193 - mrcnn_bbox_loss: 0.0389 - mrcnn_mask_loss: 0.1557 - val_loss: 0.4070 - val_rpn_class_loss: 2.7688e-04 - val_rpn_bbox_loss: 0.0908 - val_mrcnn_class_loss: 0.0060 - val_mrcnn_bbox_loss: 0.0812 - val_mrcnn_mask_loss: 0.2287\n",
      "Epoch 68/69\n",
      "300/300 [==============================] - 711s 2s/step - loss: 0.2533 - rpn_class_loss: 0.0024 - rpn_bbox_loss: 0.0406 - mrcnn_class_loss: 0.0188 - mrcnn_bbox_loss: 0.0369 - mrcnn_mask_loss: 0.1545 - val_loss: 0.3661 - val_rpn_class_loss: 6.6432e-05 - val_rpn_bbox_loss: 0.0836 - val_mrcnn_class_loss: 8.2429e-04 - val_mrcnn_bbox_loss: 0.0654 - val_mrcnn_mask_loss: 0.2162\n",
      "Epoch 69/69\n",
      "300/300 [==============================] - 711s 2s/step - loss: 0.2515 - rpn_class_loss: 0.0021 - rpn_bbox_loss: 0.0400 - mrcnn_class_loss: 0.0182 - mrcnn_bbox_loss: 0.0363 - mrcnn_mask_loss: 0.1549 - val_loss: 0.3741 - val_rpn_class_loss: 1.3762e-04 - val_rpn_bbox_loss: 0.0798 - val_mrcnn_class_loss: 3.0998e-04 - val_mrcnn_bbox_loss: 0.0603 - val_mrcnn_mask_loss: 0.2336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#with img_size = 448X448\\nmodel.train(dataset_train, dataset_val, \\n            learning_rate=opt.LEARNING_RATE, \\n            epochs=60, \\n            layers='all')\\n        \\n\\nmodel.train(dataset_train, dataset_val, \\n            learning_rate=opt.LEARNING_RATE/10, \\n            epochs=75, \\n            layers='all')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=opt.LEARNING_RATE, \n",
    "            epochs=69, \n",
    "            layers='all')\n",
    "\n",
    "\"\"\"\n",
    "#with img_size = 448X448\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=opt.LEARNING_RATE, \n",
    "            epochs=60, \n",
    "            layers='all')\n",
    "        \n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=opt.LEARNING_RATE/10, \n",
    "            epochs=75, \n",
    "            layers='all')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 100.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  checkpoints.h5/nuclei20180327T2044/mask_rcnn_nuclei_0069.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:40<00:00,  1.55s/it] \n"
     ]
    }
   ],
   "source": [
    "from skimage import morphology\n",
    "from skimage.morphology import binary_closing, binary_opening, disk, binary_dilation\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "def rle_encode(x):\n",
    "    '''\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[1]*shape[0], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape((shape[1], shape[0])).T\n",
    "\n",
    "\n",
    "dataset = NucleiDataset()\n",
    "dataset.load_nuclei(ROOT_DIR,'inference')\n",
    "dataset.prepare()\n",
    "\n",
    "\n",
    "\n",
    "class InferenceConfig(Config2):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    MEAN_PIXEL = np.array([56.02288505, 54.02376286, 54.26675248])\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=opt.MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = 'checkpoints.h5/nuclei20180327T2044/mask_rcnn_nuclei_0069.h5' #model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"checkpoints.h5/nuclei20180327T2044/mask_rcnn_nuclei_{epoch:04d}.h5\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "load test dataset one by one. Note that masks are resized (back) in model.detect\n",
    "rle2csv\n",
    "'''        \n",
    "#ImageId = []\n",
    "#EncodedPixels = []\n",
    "\n",
    "output = []\n",
    "sample_submission = pd.read_csv('stage1_sample_submission.csv')\n",
    "\n",
    "for image_id in tqdm(sample_submission.ImageId):\n",
    "    image_path = os.path.join('test_imgs', image_id + '.png')\n",
    "    \n",
    "    original_image = cv2.imread(image_path)\n",
    "    results = model.detect([original_image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    masks = r['masks']\n",
    "    \n",
    "    count = masks.shape[-1]\n",
    "    occlusion = np.logical_not(masks[:, :, -1]).astype(np.uint8)\n",
    "    \n",
    "    for i in range(count - 2, -1, -1):\n",
    "        mask = masks[:, :, i] * occlusion\n",
    "        mask_rle = rle_to_string(rle_encode(mask))\n",
    "        \n",
    "        # Sanity check\n",
    "        try:\n",
    "            rle_decode(mask_rle, original_image.shape[:-1])\n",
    "            output.append([image_id, mask_rle])\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(masks[:, :, i]))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(image_id)\n",
    "            print('---')\n",
    "        \n",
    "output_df = pd.DataFrame(output, columns=['ImageId', 'EncodedPixels'])\n",
    "output_df.to_csv('42nd_sub.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
